{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dca2d1-5fbb-47c4-b9fe-8113bd8e895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\navya\\anaconda3\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\navya\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\navya\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\navya\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\navya\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: evaluate in c:\\users\\navya\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\navya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn torch pandas evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a54cd0-db11-4a8b-90d6-521bdd39522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Configure Python logging to write a plain text file inside ./logs/\n",
    "logging.basicConfig(\n",
    "filename=\"./logs/training.log\", \n",
    "filemode=\"w\", \n",
    "format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- (Your data loading and tokenizing code would be here) ---\n",
    "\n",
    "# Define Metrics\n",
    "metric = evaluate.load(\"accuracy\") # <-- CHANGE 2: New way to load metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc11b9e8-07d5-40ce-9580-379e1f2b930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "\n",
      "Data after combining and cleaning:\n",
      "                                                text  label\n",
      "0  Re: Accurate Background Invites You to Partici...   True\n",
      "1  Re: Invitation to Walkathon ‚Äì Swachhata Pakhwa...  False\n",
      "2  Re: Seven Pages - Cozy Book Reading Session [S...  False\n",
      "3  Seven Pages - Cozy Book Reading Session [SEP] ...  False\n",
      "4  Re: Intro to GPU Design!! [SEP] \"'Elektronica'...  False\n",
      "\n",
      "Training set size: 400 (80%)\n",
      "Validation set size: 50 (10%)\n",
      "Testing set size: 50 (10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\AppData\\Local\\Temp\\ipykernel_23168\\2899193074.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['text'] = df_filtered['subject'].fillna('') + ' [SEP] ' + df_filtered['from'].fillna('') + ' [SEP] ' + df_filtered['body'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. LOAD YOUR DATASET ---\n",
    "try:\n",
    "    df = pd.read_csv('labeled_emails.csv')\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'labeled_emails.csv' is in the same directory as your notebook, or provide the full path.\")\n",
    "    # Create a dummy dataframe to prevent further errors if the file isn't found\n",
    "    df = pd.DataFrame({\n",
    "        'subject': ['test subject 1', 'test subject 2'],\n",
    "        'from': ['sender@example.com', 'another@example.com'],\n",
    "        'body': ['This is the body of the first email.', 'This is the second email body.'],\n",
    "        'label_true_if_any': [0, 1]\n",
    "    })\n",
    "    print(\"A dummy dataframe has been created to allow the code to run.\")\n",
    "\n",
    "\n",
    "# --- 2. SELECT, COMBINE, AND RENAME COLUMNS ---\n",
    "# Keep only the columns we need\n",
    "df_filtered = df[['subject', 'from', 'body', 'label_true_if_any']]\n",
    "\n",
    "# Combine the text columns into a single 'text' column for BERT\n",
    "df_filtered['text'] = df_filtered['subject'].fillna('') + ' [SEP] ' + df_filtered['from'].fillna('') + ' [SEP] ' + df_filtered['body'].fillna('')\n",
    "\n",
    "# Rename the label column to 'label', which is expected by the Hugging Face Trainer\n",
    "df_filtered = df_filtered.rename(columns={'label_true_if_any': 'label'})\n",
    "\n",
    "# Create a final, clean dataframe with only the 'text' and 'label' columns\n",
    "final_df = df_filtered[['text', 'label']]\n",
    "\n",
    "print(\"\\nData after combining and cleaning:\")\n",
    "print(final_df.head())\n",
    "\n",
    "\n",
    "# --- 3. SPLIT THE DATA (80% train, 10% validation, 10% test) ---\n",
    "train_df, temp_df = train_test_split(\n",
    "    final_df,\n",
    "    test_size=0.2,         \n",
    "    random_state=42,        \n",
    "    stratify=final_df['label'] \n",
    ")\n",
    "\n",
    "# Step B: Split the temporary set (20%) into validation (10%) and testing (10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,         \n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "# --- 4. VERIFY THE SPLIT ---\n",
    "print(f\"\\nTraining set size: {len(train_df)} ({len(train_df) / len(final_df):.0%})\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df) / len(final_df):.0%})\")\n",
    "print(f\"Testing set size: {len(test_df)} ({len(test_df) / len(final_df):.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e263e831-02d3-4e9e-a68a-8b4f5792c755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004795e25da64b72b4a7f4ef9c03164f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3881581c43d04efdaf656a6a413f0e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383fcafa8f4d430199c5443722ea1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Here's a look at the first training sample after tokenization:\n",
      "{'text': \"Internship Opportunity with the Tata Group- Win INR 2.5 lakhs- Apply now! [SEP] Tata Crucible <noreply@dare2compete.news> [SEP] Tata Crucible Campus Quiz 2025 Register Today! \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè \\ufeff Õè Hi Chunduri, Are you looking for your dream job or internship? Because Tata Group is in town with Crucible Campus Quiz 2025! Offer: Internship* with the Tata Group for the top 4 National Finalists Eligibility: Open to all full-time students pursuing a degree or post-graduation courses Exciting Prizes: Grand Cash Prize ‚Çπ2.5 Lakhs* Luxury Holiday Experience Taj Hotels worth ‚Çπ50,000* Many other prizes* Participate Now*Terms and Conditions apply. All prizes are subject to applicable tax deductions at source. Regards,Team Tata Crucible If you'd like to unsubscribe and stop receiving these emails click here\", 'label': True, '__index_level_0__': 95, 'input_ids': [101, 22676, 4495, 2007, 1996, 23236, 2177, 1011, 2663, 1999, 2099, 1016, 1012, 1019, 2474, 10023, 2015, 1011, 6611, 2085, 999, 102, 23236, 13675, 21104, 1026, 4496, 13699, 2135, 1030, 8108, 2475, 9006, 22327, 2063, 1012, 2739, 1028, 102, 23236, 13675, 21104, 3721, 19461, 16798, 2629, 4236, 2651, 999, 7632, 20979, 24979, 2072, 1010, 2024, 2017, 2559, 2005, 2115, 3959, 3105, 2030, 22676, 1029, 2138, 23236, 2177, 2003, 1999, 2237, 2007, 13675, 21104, 3721, 19461, 16798, 2629, 999, 3749, 1024, 22676, 1008, 2007, 1996, 23236, 2177, 2005, 1996, 2327, 1018, 2120, 13527, 11395, 1024, 2330, 2000, 2035, 2440, 1011, 2051, 2493, 11828, 1037, 3014, 2030, 2695, 1011, 7665, 5352, 10990, 11580, 1024, 2882, 5356, 3396, 1576, 2475, 1012, 1019, 2474, 10023, 2015, 1008, 9542, 6209, 3325, 11937, 3501, 9275, 4276, 1576, 12376, 1010, 2199, 1008, 2116, 2060, 11580, 1008, 5589, 2085, 1008, 3408, 1998, 3785, 6611, 1012, 2035, 11580, 2024, 3395, 2000, 12711, 4171, 2139, 16256, 2015, 2012, 3120, 1012, 12362, 1010, 2136, 23236, 13675, 21104, 2065, 2017, 1005, 1040, 2066, 2000, 4895, 6342, 5910, 26775, 20755, 1998, 2644, 4909, 2122, 22028, 11562, 2182, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 1. LOAD THE TOKENIZER ---\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 2. CONVERT PANDAS TO HUGGING FACE DATASET OBJECT ---\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- 3. CREATE A TOKENIZATION FUNCTION ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# --- 4. APPLY THE TOKENIZER TO ALL DATASETS ---\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(\"Here's a look at the first training sample after tokenization:\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dae97a-3c5d-4cd6-b666-e2117f88386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Based on your output ('True'/'False'), you have 2 labels.\n",
    "num_labels = 2 \n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623714c2-3772-4b75-b888-8b26459b8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              \n",
    "    num_train_epochs=5,                  \n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=16,       \n",
    "    warmup_steps=500,                    \n",
    "    weight_decay=0.01,                  \n",
    "    logging_dir='./logs',                \n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",               \n",
    "    load_best_model_at_end=True,         \n",
    "    report_to=[\"tensorboard\"],           \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec2b297-8caf-444b-8470-8b284c3747ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Training Arguments, and Metrics are now defined.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"Model, Training Arguments, and Metrics are now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89bfe15-8726-4a9d-b5c4-0f61df6c610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback\n",
    "\n",
    "# ---- Callback that writes logs to ./logs/training.log using your existing `logger` ----\n",
    "class LogCallback(TrainerCallback):\n",
    "    \"\"\"Write Trainer logs (loss, lr, eval metrics) to ./logs/training.log\"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        step = getattr(state, \"global_step\", None)\n",
    "        if logs:\n",
    "            # logger comes from the logging setup you added in Step 1\n",
    "            logger.info(f\"Step {step} - {logs}\")\n",
    "\n",
    "# ---- Create the Trainer with the LogCallback attached ----\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[LogCallback]   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68adb9a9-102c-477e-93ad-f428fa8a0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 3:04:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.603863</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.413679</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.439562</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.453821</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.352705</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.3756684126853943, metrics={'train_runtime': 11101.9371, 'train_samples_per_second': 0.18, 'train_steps_per_second': 0.023, 'total_flos': 526222110720000.0, 'train_loss': 0.3756684126853943, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6247a8f-200e-4a00-91da-c54daf3366af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Classifier Model\\\\tokenizer_config.json',\n",
       " 'Classifier Model\\\\special_tokens_map.json',\n",
       " 'Classifier Model\\\\vocab.txt',\n",
       " 'Classifier Model\\\\added_tokens.json',\n",
       " 'Classifier Model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In train_classifier.ipynb, at the very end:\n",
    "save_directory = \"Classifier Model\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a2d573-896e-49a3-93f1-c42eae76af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Set Performance ---\n",
      "{'eval_loss': 0.5766546726226807, 'eval_accuracy': 0.9, 'eval_runtime': 42.9622, 'eval_samples_per_second': 1.164, 'eval_steps_per_second': 0.093, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Run a final evaluation on the test set\n",
    "final_evaluation_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\n--- Final Test Set Performance ---\")\n",
    "print(final_evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9578252b-18af-4f4a-9dcf-ba47ff2c82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate at the end and log metrics\n",
    "metrics = trainer.evaluate()\n",
    "logger.info(f\"Final evaluation metrics: {metrics}\")\n",
    "\n",
    "# Save predictions to ./logs/predictions.csv\n",
    "preds_output = trainer.predict(tokenized_val_dataset)\n",
    "logits, labels = preds_output.predictions, preds_output.label_ids\n",
    "preds = logits.argmax(axis=-1)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({'label': labels.flatten(), 'pred': preds.flatten()}).to_csv('./logs/predictions.csv', index=False)\n",
    "logger.info(\"Saved predictions to ./logs/predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de315b0d-20c7-4ed7-b56c-d4fded878ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction results to 'prediction_results.csv'\n",
      "                                                  text  label  true_label  \\\n",
      "92   Fwd: Hackathon Submission Deadline Approaching...   True           1   \n",
      "399  Re: üì∏ Join Camera Handling: 101 ‚Äì Photography ...  False           0   \n",
      "234  Re: Implementation of 24√ó7 Library Access as P...  False           0   \n",
      "183  New assignment: \"Filters\" [SEP] \"Syed (Classro...   True           1   \n",
      "304  Fwd: TRIALS FOR INTER IIT LAWN TENNIS [SEP] \"'...  False           0   \n",
      "\n",
      "     predicted_label  \n",
      "92                 1  \n",
      "399                0  \n",
      "234                0  \n",
      "183                1  \n",
      "304                0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use the trainer to get predictions on the test set\n",
    "predictions_output = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# The raw predictions are logits, so we take the argmax to get the predicted class (0 or 1)\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Create a new DataFrame for easy comparison\n",
    "results_df = test_df.copy()\n",
    "results_df['true_label'] = true_labels\n",
    "results_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('prediction_results.csv', index=False)\n",
    "\n",
    "print(\"Saved prediction results to 'prediction_results.csv'\")\n",
    "print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
