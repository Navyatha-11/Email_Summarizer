{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dca2d1-5fbb-47c4-b9fe-8113bd8e895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\navya\\anaconda3\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\navya\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\navya\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\navya\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\navya\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: evaluate in c:\\users\\navya\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\navya\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\navya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\navya\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn torch pandas evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a54cd0-db11-4a8b-90d6-521bdd39522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Configure Python logging to write a plain text file inside ./logs/\n",
    "logging.basicConfig(\n",
    "filename=\"./logs/training.log\", \n",
    "filemode=\"w\", \n",
    "format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- (Your data loading and tokenizing code would be here) ---\n",
    "\n",
    "# Define Metrics\n",
    "metric = evaluate.load(\"accuracy\") # <-- CHANGE 2: New way to load metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc11b9e8-07d5-40ce-9580-379e1f2b930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "\n",
      "Data after combining and cleaning:\n",
      "                                                text  label\n",
      "0  Re: Accurate Background Invites You to Partici...   True\n",
      "1  Re: Invitation to Walkathon – Swachhata Pakhwa...  False\n",
      "2  Re: Seven Pages - Cozy Book Reading Session [S...  False\n",
      "3  Seven Pages - Cozy Book Reading Session [SEP] ...  False\n",
      "4  Re: Intro to GPU Design!! [SEP] \"'Elektronica'...  False\n",
      "\n",
      "Training set size: 400 (80%)\n",
      "Validation set size: 50 (10%)\n",
      "Testing set size: 50 (10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\AppData\\Local\\Temp\\ipykernel_23168\\2899193074.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['text'] = df_filtered['subject'].fillna('') + ' [SEP] ' + df_filtered['from'].fillna('') + ' [SEP] ' + df_filtered['body'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. LOAD YOUR DATASET ---\n",
    "try:\n",
    "    df = pd.read_csv('labeled_emails.csv')\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'labeled_emails.csv' is in the same directory as your notebook, or provide the full path.\")\n",
    "    # Create a dummy dataframe to prevent further errors if the file isn't found\n",
    "    df = pd.DataFrame({\n",
    "        'subject': ['test subject 1', 'test subject 2'],\n",
    "        'from': ['sender@example.com', 'another@example.com'],\n",
    "        'body': ['This is the body of the first email.', 'This is the second email body.'],\n",
    "        'label_true_if_any': [0, 1]\n",
    "    })\n",
    "    print(\"A dummy dataframe has been created to allow the code to run.\")\n",
    "\n",
    "\n",
    "# --- 2. SELECT, COMBINE, AND RENAME COLUMNS ---\n",
    "# Keep only the columns we need\n",
    "df_filtered = df[['subject', 'from', 'body', 'label_true_if_any']]\n",
    "\n",
    "# Combine the text columns into a single 'text' column for BERT\n",
    "df_filtered['text'] = df_filtered['subject'].fillna('') + ' [SEP] ' + df_filtered['from'].fillna('') + ' [SEP] ' + df_filtered['body'].fillna('')\n",
    "\n",
    "# Rename the label column to 'label', which is expected by the Hugging Face Trainer\n",
    "df_filtered = df_filtered.rename(columns={'label_true_if_any': 'label'})\n",
    "\n",
    "# Create a final, clean dataframe with only the 'text' and 'label' columns\n",
    "final_df = df_filtered[['text', 'label']]\n",
    "\n",
    "print(\"\\nData after combining and cleaning:\")\n",
    "print(final_df.head())\n",
    "\n",
    "\n",
    "# --- 3. SPLIT THE DATA (80% train, 10% validation, 10% test) ---\n",
    "train_df, temp_df = train_test_split(\n",
    "    final_df,\n",
    "    test_size=0.2,         \n",
    "    random_state=42,        \n",
    "    stratify=final_df['label'] \n",
    ")\n",
    "\n",
    "# Step B: Split the temporary set (20%) into validation (10%) and testing (10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,         \n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "# --- 4. VERIFY THE SPLIT ---\n",
    "print(f\"\\nTraining set size: {len(train_df)} ({len(train_df) / len(final_df):.0%})\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df) / len(final_df):.0%})\")\n",
    "print(f\"Testing set size: {len(test_df)} ({len(test_df) / len(final_df):.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e263e831-02d3-4e9e-a68a-8b4f5792c755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004795e25da64b72b4a7f4ef9c03164f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3881581c43d04efdaf656a6a413f0e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383fcafa8f4d430199c5443722ea1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Here's a look at the first training sample after tokenization:\n",
      "{'text': \"Internship Opportunity with the Tata Group- Win INR 2.5 lakhs- Apply now! [SEP] Tata Crucible <noreply@dare2compete.news> [SEP] Tata Crucible Campus Quiz 2025 Register Today! \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ \\ufeff ͏ Hi Chunduri, Are you looking for your dream job or internship? Because Tata Group is in town with Crucible Campus Quiz 2025! Offer: Internship* with the Tata Group for the top 4 National Finalists Eligibility: Open to all full-time students pursuing a degree or post-graduation courses Exciting Prizes: Grand Cash Prize ₹2.5 Lakhs* Luxury Holiday Experience Taj Hotels worth ₹50,000* Many other prizes* Participate Now*Terms and Conditions apply. All prizes are subject to applicable tax deductions at source. Regards,Team Tata Crucible If you'd like to unsubscribe and stop receiving these emails click here\", 'label': True, '__index_level_0__': 95, 'input_ids': [101, 22676, 4495, 2007, 1996, 23236, 2177, 1011, 2663, 1999, 2099, 1016, 1012, 1019, 2474, 10023, 2015, 1011, 6611, 2085, 999, 102, 23236, 13675, 21104, 1026, 4496, 13699, 2135, 1030, 8108, 2475, 9006, 22327, 2063, 1012, 2739, 1028, 102, 23236, 13675, 21104, 3721, 19461, 16798, 2629, 4236, 2651, 999, 7632, 20979, 24979, 2072, 1010, 2024, 2017, 2559, 2005, 2115, 3959, 3105, 2030, 22676, 1029, 2138, 23236, 2177, 2003, 1999, 2237, 2007, 13675, 21104, 3721, 19461, 16798, 2629, 999, 3749, 1024, 22676, 1008, 2007, 1996, 23236, 2177, 2005, 1996, 2327, 1018, 2120, 13527, 11395, 1024, 2330, 2000, 2035, 2440, 1011, 2051, 2493, 11828, 1037, 3014, 2030, 2695, 1011, 7665, 5352, 10990, 11580, 1024, 2882, 5356, 3396, 1576, 2475, 1012, 1019, 2474, 10023, 2015, 1008, 9542, 6209, 3325, 11937, 3501, 9275, 4276, 1576, 12376, 1010, 2199, 1008, 2116, 2060, 11580, 1008, 5589, 2085, 1008, 3408, 1998, 3785, 6611, 1012, 2035, 11580, 2024, 3395, 2000, 12711, 4171, 2139, 16256, 2015, 2012, 3120, 1012, 12362, 1010, 2136, 23236, 13675, 21104, 2065, 2017, 1005, 1040, 2066, 2000, 4895, 6342, 5910, 26775, 20755, 1998, 2644, 4909, 2122, 22028, 11562, 2182, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 1. LOAD THE TOKENIZER ---\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- 2. CONVERT PANDAS TO HUGGING FACE DATASET OBJECT ---\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- 3. CREATE A TOKENIZATION FUNCTION ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# --- 4. APPLY THE TOKENIZER TO ALL DATASETS ---\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(\"Here's a look at the first training sample after tokenization:\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dae97a-3c5d-4cd6-b666-e2117f88386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Based on your output ('True'/'False'), you have 2 labels.\n",
    "num_labels = 2 \n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623714c2-3772-4b75-b888-8b26459b8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              \n",
    "    num_train_epochs=5,                  \n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=16,       \n",
    "    warmup_steps=500,                    \n",
    "    weight_decay=0.01,                  \n",
    "    logging_dir='./logs',                \n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",               \n",
    "    load_best_model_at_end=True,         \n",
    "    report_to=[\"tensorboard\"],           \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec2b297-8caf-444b-8470-8b284c3747ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Training Arguments, and Metrics are now defined.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"Model, Training Arguments, and Metrics are now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89bfe15-8726-4a9d-b5c4-0f61df6c610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback\n",
    "\n",
    "# ---- Callback that writes logs to ./logs/training.log using your existing `logger` ----\n",
    "class LogCallback(TrainerCallback):\n",
    "    \"\"\"Write Trainer logs (loss, lr, eval metrics) to ./logs/training.log\"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        step = getattr(state, \"global_step\", None)\n",
    "        if logs:\n",
    "            # logger comes from the logging setup you added in Step 1\n",
    "            logger.info(f\"Step {step} - {logs}\")\n",
    "\n",
    "# ---- Create the Trainer with the LogCallback attached ----\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[LogCallback]   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68adb9a9-102c-477e-93ad-f428fa8a0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 3:04:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.603863</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.413679</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.439562</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.453821</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.352705</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.3756684126853943, metrics={'train_runtime': 11101.9371, 'train_samples_per_second': 0.18, 'train_steps_per_second': 0.023, 'total_flos': 526222110720000.0, 'train_loss': 0.3756684126853943, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6247a8f-200e-4a00-91da-c54daf3366af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Classifier Model\\\\tokenizer_config.json',\n",
       " 'Classifier Model\\\\special_tokens_map.json',\n",
       " 'Classifier Model\\\\vocab.txt',\n",
       " 'Classifier Model\\\\added_tokens.json',\n",
       " 'Classifier Model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In train_classifier.ipynb, at the very end:\n",
    "save_directory = \"Classifier Model\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a2d573-896e-49a3-93f1-c42eae76af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Set Performance ---\n",
      "{'eval_loss': 0.5766546726226807, 'eval_accuracy': 0.9, 'eval_runtime': 42.9622, 'eval_samples_per_second': 1.164, 'eval_steps_per_second': 0.093, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Run a final evaluation on the test set\n",
    "final_evaluation_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\n--- Final Test Set Performance ---\")\n",
    "print(final_evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9578252b-18af-4f4a-9dcf-ba47ff2c82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate at the end and log metrics\n",
    "metrics = trainer.evaluate()\n",
    "logger.info(f\"Final evaluation metrics: {metrics}\")\n",
    "\n",
    "# Save predictions to ./logs/predictions.csv\n",
    "preds_output = trainer.predict(tokenized_val_dataset)\n",
    "logits, labels = preds_output.predictions, preds_output.label_ids\n",
    "preds = logits.argmax(axis=-1)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({'label': labels.flatten(), 'pred': preds.flatten()}).to_csv('./logs/predictions.csv', index=False)\n",
    "logger.info(\"Saved predictions to ./logs/predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de315b0d-20c7-4ed7-b56c-d4fded878ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navya\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction results to 'prediction_results.csv'\n",
      "                                                  text  label  true_label  \\\n",
      "92   Fwd: Hackathon Submission Deadline Approaching...   True           1   \n",
      "399  Re: 📸 Join Camera Handling: 101 – Photography ...  False           0   \n",
      "234  Re: Implementation of 24×7 Library Access as P...  False           0   \n",
      "183  New assignment: \"Filters\" [SEP] \"Syed (Classro...   True           1   \n",
      "304  Fwd: TRIALS FOR INTER IIT LAWN TENNIS [SEP] \"'...  False           0   \n",
      "\n",
      "     predicted_label  \n",
      "92                 1  \n",
      "399                0  \n",
      "234                0  \n",
      "183                1  \n",
      "304                0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use the trainer to get predictions on the test set\n",
    "predictions_output = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# The raw predictions are logits, so we take the argmax to get the predicted class (0 or 1)\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Create a new DataFrame for easy comparison\n",
    "results_df = test_df.copy()\n",
    "results_df['true_label'] = true_labels\n",
    "results_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('prediction_results.csv', index=False)\n",
    "\n",
    "print(\"Saved prediction results to 'prediction_results.csv'\")\n",
    "print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
